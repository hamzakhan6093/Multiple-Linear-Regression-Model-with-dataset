import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt


#Importing a dataset 
dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 4].values



# Categorial data 
# Encoding in the indepenent variable
from sklearn.preprocessing import LabelEncoder 
Labelencoder_X= LabelEncoder()
X[:, 3] = Labelencoder_X.fit_transform(X[:, 3])  #Here we have to put the index of our categorial column

# To create Dummy variable 
from sklearn.preprocessing import OneHotEncoder
# onehotencoder = OneHotEncoder(categories_features = [3]) #we have to apply categorical feature in column 3 
# X = onehotencoder.fit_transform(X).toarray()
from sklearn.compose import ColumnTransformer
# Country column
Country_Column = ColumnTransformer([("State", OneHotEncoder(), [3])], remainder = 'passthrough')
X = Country_Column.fit_transform(X)

# Avoid Dummy variable trap 
X = X[:, 1:]

# testing and training a data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Training our model 
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predicting our result
y_predict = regressor.predict(X_test)


# Build an optimal model sing elimination
import statsmodels.regression.linear_model as sm
# import statsmodels.api as sm
# X = np.append(arr = X, values = np.ones((50,1)).astype(int), axis = 1) #This equestion is to add the column in the end
#0 is used for a row and 1 is usd for a column and we are adding it at to end of our dataset but we need it in the start because 
# We know that equation Y= b0*X0,b1*X1+.....+ B5*X5 
X = np.append(arr = np.ones((50,1)).astype(int), values = X, axis = 1)

# Here we are going to apply our backward Elimination Algorithm to ready our model.
X_opt = X[:, [0,1,2,3,4,5]] #X_opt array has a dtype of object and this may be causing an error. Try changing it to float.
X_opt = np.array(X_opt, dtype=float)
regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()

# According to backward Eimination Algorithm until the P>SL, remove the highest predictor so, 

X_opt = X[:, [0,1,3,4,5]] 
X_opt = np.array(X_opt, dtype=float)
regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()

# Again we have the x1 which is higher then our significance value = 50%

X_opt = X[:, [0,3,4,5]] #X_opt array has a dtype of object and this may be causing an error. Try changing it to float.
X_opt = np.array(X_opt, dtype=float)
regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()

# Again we have the x4 or index 4 which is higher then our significance value = 50%

X_opt = X[:, [0,3,5]] #X_opt array has a dtype of object and this may be causing an error. Try changing it to float.
X_opt = np.array(X_opt, dtype=float)
regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()

# Again we have the X5 or index 5 which is higher then our significance value = 50%

X_opt = X[:, [0,3]] #X_opt array has a dtype of object and this may be causing an error. Try changing it to float.
X_opt = np.array(X_opt, dtype=float)
regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()
regressor_OLS.summary()
